# ğŸ§  AI Agent Lab - Backend API

Este repositorio contiene la **capa API REST** que conecta el frontend del laboratorio con el entorno experimental de ejecuciÃ³n de modelos LLM (LangGraph, LangChain, modelos locales, etc).

---

## ğŸ¯ Objetivo

- Recibir prompts desde el frontend (POST `/api/v1/execute`)
- Ejecutar tareas en segundo plano con el laboratorio (`AI-Agent-Lab`)
- Devolver outputs, flujos y mÃ©tricas
- (Futuro) Emitir progreso a WebSocket y persistir en PostgreSQL

---

## ğŸ› ï¸ Stack TecnolÃ³gico

- **FastAPI** â€” Framework principal (ASGI)
- **Python 3.10+**
- **Docker (opcional)** â€” Para entorno local completo
- **Redis (futuro)** â€” Para WebSocket Pub/Sub
- **PostgreSQL (futuro)** â€” Para historial

---

## ğŸ“ Estructura del proyecto

backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # Entrypoint FastAPI
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ v1/
â”‚ â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”‚ â””â”€â”€ execute.py # POST /api/v1/execute
â”‚ â”‚ â”œâ”€â”€ schemas/
â”‚ â”‚ â”‚ â”œâ”€â”€ execution.py # ExecutionRequest, ExecutionResult
â”‚ â”‚ â”‚ â””â”€â”€ types.py # FlowState, ExecutionMetrics, etc.
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ config.py # ConfiguraciÃ³n desde .env
â”‚ â”‚ â””â”€â”€ redis.py # Cliente Redis (futuro)
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â””â”€â”€ executor.py # Invoca el orquestador del lab
â”‚ â””â”€â”€ sockets/
â”‚ â””â”€â”€ manager.py # WebSocket listener (opcional)
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_execute.py # Test del endpoint
â””â”€â”€ README.md


3. Ejecutar el servidor
venv\Scripts\Activate.ps1
uvicorn app.main:app --reload



4. Ejecutar una pregunta desde archivo con CURL
curl -X POST "http://localhost:8000/api/v1/execute" \
  -H "Content-Type: application/json" \
  -d "{\"prompt\": \"Dime 3 nÃºmeros primos\", \"model\": \"mistral7b\"}"

  Invoke-RestMethod -Uri "http://localhost:8000/api/v1/execute" -Method POST -ContentType "application/json" -Body '{"prompt": "Que es Boca Juniors", "model": "mistral7b"}'


Backend â†’ API Call â†’ Lab Service (/orchestrate endpoint)
Backend â†’ API Call â†’ Lab Service (/inference endpoint)

ğŸ”„ URLs resultantes:

POST /inference/ - Simple LLM (como antes)
POST /orchestrate/ - LangGraph orchestrator (nuevo)
GET /orchestrate/ - Health check del orchestrator